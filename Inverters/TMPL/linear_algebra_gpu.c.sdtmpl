/***************************************************************************\
* Copyright (c) 2008, Claudio Pica, Ari Hietanen and Ulrik SÃ¸ndergaard      *   
* All rights reserved.                                                      * 
\***************************************************************************/

#include "communications.h"
#include "linear_algebra_gpu.cu"
#include "gpu.h"

#define _TOCOMPLEX(sp) ((_COMPLEX *)(sp))
#define _NCOM (sizeof(_SPINOR_TYPE)/sizeof(_COMPLEX))

/* Re <s1,s2> */
double _FUNC(spinor_field_prod_re)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  _REAL res;
  _REAL * resGPU; // one number
  _REAL * resField; // resulting scalar field when dotting each spinor
  
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1; 
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  
  // Allocate temp field
  cudaMalloc((void **) &resField,N*sizeof(_REAL));
  cudaMalloc((void **) &resGPU,sizeof(_REAL));
  
  spinor_field_prod_re_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),resField,N);
  global_sum_gpu<<<1,BLOCK_SIZE>>>(resField,resGPU,N);
  
  // Copy result to device
  cudaMemcpy(&res,resGPU,sizeof(_REAL),cudaMemcpyDeviceToHost);
  
  // deallocating
  cudaFree(resField);
  cudaFree(resGPU);
  
  return (double) res;
}

/* Im <s1,s2> */
double _FUNC(spinor_field_prod_im)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  _REAL res;
  _REAL* resGPU;
  _REAL* resField;
  
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  cudaMalloc((void **) &resField,N*sizeof(_REAL));
  cudaMalloc((void **) &resGPU,sizeof(_REAL));
  
  spinor_field_prod_im_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),resField,N);
  global_sum_gpu<<<1,BLOCK_SIZE>>>(resField,resGPU,N);
  cudaMemcpy(&res,resGPU,sizeof(_REAL),cudaMemcpyDeviceToHost);
  cudaFree(resField);
  cudaFree(resGPU);
  return (double) res;
}

/* <s1,s2> */
complex _FUNC(spinor_field_prod)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  _COMPLEX res;			// One number on cpu
  complex res_dbl;
  _COMPLEX * resGPU;   	// one number
  _COMPLEX * resField; 	// resulting scalar field when dotting each spinor

  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1; 
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
 
  // Allocate temp field
  cudaMalloc((void **) &resField,N*sizeof(_COMPLEX));
  cudaMalloc((void **) &resGPU,sizeof(_COMPLEX));
 
  // Not defined yet
//  complex_field_zero_gpu<<<grid,BLOCK_SIZE>>>(resField,N);
 
  // Calling kernel
  spinor_field_prod_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),resField,N);
  global_sum_complex_gpu<<<1,BLOCK_SIZE>>>(resField,resGPU,N);
    
  // Copy result to device
  cudaMemcpy(&res,resGPU,sizeof(_COMPLEX),cudaMemcpyDeviceToHost);
    
  res_dbl.re=(double)res.re; res_dbl.im=(double)res.im; // CASTING TO DBL COMPLEX
  return res_dbl;		

}

/* Re <g5*s1,s2> */
double _FUNC(spinor_field_g5_prod_re)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  _REAL res;
  _REAL* resGPU;
  _REAL* resField;
  
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N*=_NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  cudaMalloc((void **) &resField,N*sizeof(_REAL));
  cudaMalloc((void **) &resGPU,sizeof(_REAL));

  spinor_field_g5_prod_re_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),resField,N);
  global_sum_gpu<<<1,BLOCK_SIZE>>>(resField,resGPU,N);  
  cudaMemcpy(&res,resGPU,sizeof(_REAL),cudaMemcpyDeviceToHost);
  cudaFree(resField);
  cudaFree(resGPU);
  CudaCheckError();   

  return (double) res;
}

/* Im <g5*s1,s2> */
double _FUNC(spinor_field_g5_prod_im)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  _REAL res;
  _REAL * resGPU; // one number
  _REAL * resField; // resulting scalar field when dotting each spinor

  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);

  // Allocate temp field
  cudaMalloc((void **) &resField,N*sizeof(_REAL));
  cudaMalloc((void **) &resGPU,sizeof(_REAL));

  // Invoke kernel
  spinor_field_g5_prod_im_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),resField,N);
  global_sum_gpu<<<1,BLOCK_SIZE>>>(resField,resGPU,N);
    
  // Copy result to device
  cudaMemcpy(&res,resGPU,sizeof(_REAL),cudaMemcpyDeviceToHost);
    
  // deallocating
  cudaFree(resField);
  cudaFree(resGPU);
    
  return (double) res;

}

/* Re <s1,s1> */
double _FUNC(spinor_field_sqnorm)(_SPINOR_FIELD_TYPE *s1)
{
  _REAL res;
  _REAL* resGPU;
  _REAL* resField;
  
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  cudaMalloc((void **) &resField,N*sizeof(_REAL));
  cudaMalloc((void **) &resGPU,sizeof(_REAL));

  spinor_field_sqnorm_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),resField,N);
  global_sum_gpu<<<1,BLOCK_SIZE>>>(resField,resGPU,N);  
  cudaMemcpy(&res,resGPU,sizeof(_REAL),cudaMemcpyDeviceToHost);
  cudaFree(resField);
  cudaFree(resGPU);
  CudaCheckError();   
  return (double) res;
}

/* s1+=r*s2 r real */
void _FUNC(spinor_field_mul_add_assign)(_SPINOR_FIELD_TYPE *s1, double r, _SPINOR_FIELD_TYPE *s2)
{
  _REAL rr=(_REAL) r;
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid=N/BLOCK_SIZE + ((N%BLOCK_SIZE == 0) ? 0 : 1);

  spinor_field_mul_add_assign_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),rr,_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);

  CudaCheckError();
}

/* s1+=c*s2 c complex */
void _FUNC(spinor_field_mulc_add_assign)(_SPINOR_FIELD_TYPE *s1, complex c, _SPINOR_FIELD_TYPE *s2)
{
  _COMPLEX c1;
  c1.re=(_REAL)c.re; c1.im=(_REAL)c.im; 
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_mulc_add_assign_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),c1,_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError();
}

/* s1=r*s2 */
void _FUNC(spinor_field_mul)(_SPINOR_FIELD_TYPE *s1, double r, _SPINOR_FIELD_TYPE *s2)
{
  _REAL rr=(_REAL) r;
  
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  
  spinor_field_mul_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),rr,_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError();
}

/* s1=c*s2 c complex */
void _FUNC(spinor_field_mulc)(_SPINOR_FIELD_TYPE *s1, complex c, _SPINOR_FIELD_TYPE *s2)
{
  _COMPLEX c1;
  c1.re=(_REAL)c.re; c1.im=(_REAL)c.im; 
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_mulc_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),c1,_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError();
}

/* r=s1+s2 */
void _FUNC(spinor_field_add)(_SPINOR_FIELD_TYPE *r, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_add_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(r)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError();
}

/* r=s1-s2 */
void _FUNC(spinor_field_sub)(_SPINOR_FIELD_TYPE *r, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_sub_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(r)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError();
}

/* s1+=s2 */
void _FUNC(spinor_field_add_assign)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  
  spinor_field_add_assign_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError();
}

/* s1-=s2 */
void _FUNC(spinor_field_sub_assign)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_sub_assign_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError();
}

/* s1=0 */
void _FUNC(spinor_field_zero)(_SPINOR_FIELD_TYPE *s1)
{
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_zero_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),N);
  CudaCheckError();
}

/* s1=-s2 */
void _FUNC(spinor_field_minus)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_minus_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError();
}

/* s1=r1*s2+r2*s3 */
void _FUNC(spinor_field_lc)(_SPINOR_FIELD_TYPE *s1, double r1, _SPINOR_FIELD_TYPE *s2, double r2, _SPINOR_FIELD_TYPE *s3)
{
  _REAL rr1= (_REAL) r1;
  _REAL rr2= (_REAL) r2;

  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_lc_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),rr1,_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),rr2,_TOCOMPLEX(START_SP_ADDRESS_GPU(s3)),N);
  CudaCheckError();
}

/* s1+=r1*s2+r2*s3 */
void _FUNC(spinor_field_lc_add_assign)(_SPINOR_FIELD_TYPE *s1, double r1, _SPINOR_FIELD_TYPE *s2, double r2, _SPINOR_FIELD_TYPE *s3)
{
  _REAL rr1=(_REAL)r1;
  _REAL rr2=(_REAL)r2;
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_lc_add_assign_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),rr1,_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),rr2,_TOCOMPLEX(START_SP_ADDRESS_GPU(s3)),N);
  CudaCheckError();
}

/* s1=cd1*s2+cd2*s3 cd1, cd2 complex*/
void _FUNC(spinor_field_clc)(_SPINOR_FIELD_TYPE *s1, complex cd1, _SPINOR_FIELD_TYPE *s2, complex cd2, _SPINOR_FIELD_TYPE *s3)
{ 
  _COMPLEX c1, c2;
  c1.re=(_REAL)cd1.re; c1.im=(_REAL)cd1.im; 
  c2.re=(_REAL)cd2.re; c2.im=(_REAL)cd2.im;
   
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_clc_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),c1,_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),c2,_TOCOMPLEX(START_SP_ADDRESS_GPU(s3)),N);
  CudaCheckError();
}

/* s1+=cd1*s2+cd2*s3 cd1, cd2 complex*/
void _FUNC(spinor_field_clc_add_assign)(_SPINOR_FIELD_TYPE *s1, complex cd1, _SPINOR_FIELD_TYPE *s2, complex cd2, _SPINOR_FIELD_TYPE *s3)
{
  _COMPLEX c1, c2;
  c1.re=(_REAL)cd1.re; c1.im=(_REAL)cd1.im; 
  c2.re=(_REAL)cd2.re; c2.im=(_REAL)cd2.im;
   
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_clc_add_assign_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),c1,_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),c2,_TOCOMPLEX(START_SP_ADDRESS_GPU(s3)),N);
  CudaCheckError(); 
}

/* s1=g5*s2  */
void _FUNC(spinor_field_g5)(_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  spinor_field_g5_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError(); 
}

/* s1=g5*s1  */
void _FUNC(spinor_field_g5_assign)(_SPINOR_FIELD_TYPE *s1)
{
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  
  spinor_field_g5_assign_gpu<<<grid,BLOCK_SIZE>>>(_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),N);
  CudaCheckError(); 

}


/* tools per eva.c  */
void _FUNC(spinor_field_lc1)(double c1, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  _REAL cc1=(_REAL)c1;
  
  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  
  spinor_field_lc1_gpu<<<grid,BLOCK_SIZE>>>(cc1,_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError(); 

}


void _FUNC(spinor_field_lc2)(double c1, double c2, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)
{
  _REAL r1=(_REAL)c1;
  _REAL r2=(_REAL)c2;

  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  
  spinor_field_lc2_gpu<<<grid,BLOCK_SIZE>>>(r1,r2,_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),N);
  CudaCheckError(); 
}


void _FUNC(spinor_field_lc3)(double c1, double c2, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2, _SPINOR_FIELD_TYPE *s3)
{
  /* c1=-c1; c2=-c2; */
  _REAL cc1=-(_REAL)c1;
  _REAL cc2=-(_REAL)c2;
   

  int N = s1->type->master_end[0] -  s1->type->master_start[0] + 1;
  N *= _NCOM;  
  int grid = N/BLOCK_SIZE + ((N % BLOCK_SIZE == 0) ? 0 : 1);
  
  spinor_field_lc3_gpu<<<grid,BLOCK_SIZE>>>(cc1,cc2,_TOCOMPLEX(START_SP_ADDRESS_GPU(s1)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s2)),_TOCOMPLEX(START_SP_ADDRESS_GPU(s3)),N);
  CudaCheckError(); 
   
}


#undef _TOCOMPLEX
#undef _NCOM 
